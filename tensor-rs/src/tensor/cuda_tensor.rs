
// There are several rust library built around cuda:
// 1. There are nightly rust support and ptx support to write cuda kernel in rust.
// 2. There are cuda library wrapper for cuda ffi
// 3. There are high leverl wrapper for cuda.
//
// https://github.com/rust-cuda is the ffi wrapper
// https://github.com/bheisler/RustaCUDA is a high level wrapper, based on cuda-sys from https://github.com/rust-cuda
// https://github.com/denzp/rust-ptx-builder is used in build.rs to compile rust to ptx
// https://github.com/spearow/juice rely on a generated ffi, which is generated by a unknonw process!!!.

use cuda11_cudart_sys::{self, cudaMalloc, cudaStreamCreate, cudaMemcpy, cudaStreamSynchronize, cudaFree, cudaStreamDestroy, cudaMemcpyKind};

pub struct CudaTensor<T> {
    device_data: *mut f32,
    dim: Vec<usize>,
    mm_data: Vec<f32>,
}

impl CudaTensor {
    fn new() -> CudaTensor {
        CudaTensor {
            device_data: std::ptr::null_mut(),
            dim: Vec::new(),
            mm_data: Vec::new(),
        }
    }
    fn new_raw(data: &[f32], shape: &[usize]) -> CudaTensor {
        
        let mut device_data: *mut f32 = std::ptr::null_mut();
        let elems: usize = shape.iter().product();
        if elems != data.len() {
            panic!();
        }

        unsafe {
            println!("cudaMalloc");
            checkCudaStatus(cudaMalloc(&mut device_data as *mut _ as *mut _,
                                       std::mem::size_of::<f32>()*elems));
            println!("cudaMemcpy");
            cudaMemcpy(device_data as *mut _,
                       data.as_ptr() as *mut _,
                       std::mem::size_of::<f32>()*elems,
                       cudaMemcpyKind::cudaMemcpyHostToDevice);
        }
            
        CudaTensor {
            device_data: device_data,
            dim: shape.to_vec(),
            mm_data: data.to_vec(),
        }
    }
        
    fn _sync(&mut self) {
        let elems: usize = self.dim.iter().product();
        
        unsafe {
            cudaMemcpy(self.mm_data.as_mut_ptr() as *mut _,
                       self.device_data as *mut _,
                       std::mem::size_of::<f32>()*elems,
                       cudaMemcpyKind::cudaMemcpyDeviceToHost);
        }
    }
}

impl Drop for CudaTensor {
    fn drop(&mut self) {
        if self.device_data != std::ptr::null_mut() {
            unsafe {
                println!("cudaFree");
                checkCudaStatus(cudaFree(self.device_data as _));                    
            }
        }
    }
}

impl std::fmt::Debug for CudaTensor {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {

        write!(f, "{:?}\n", self.dim)?;
        write!(f, "{:?}", self.mm_data)

    }
}

//impl<T> Clone for CudaTensor<T> where T: num_traits::Float {
//    fn clone(&self) -> Self {
//        CudaTensor {
//            
//        }
//    }
//}


#[cfg(all(test, feature = "use-cuda"))]
mod tests {
    use super::*;

    #[test]
    fn cuda_memcpy() {
        let mut input = CudaTensor::new_raw(&vec![1., 2., 3., 4., 5., 6., 7., 8., 9.],
                                            &vec![1, 1, 3, 3]);
        input._sync();
        println!("{:?}", input);
    }

}
